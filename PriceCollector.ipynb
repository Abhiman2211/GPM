{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import urllib\n",
    "import datetime\n",
    "import os\n",
    "from pytz import timezone\n",
    "import pytz\n",
    "from pathlib import Path\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCurl = \"https://www.dukascopy.com/swiss/english/home/?utm_source=freeserv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validatePath(path):\n",
    "    \"\"\"\n",
    "    Checks if path refers to a valid directory. If not an exception is raised.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        os.path.isfile(filename)\n",
    "    except:\n",
    "        raise NotADirectoryError(\"The specified path does not exist or is invalid. The data will be collected and stored in the current working directory in the file: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmtToUtc(time):\n",
    "    utcTime = time.astimezone(timezone(\"UTC\"))\n",
    "    return utcTime.strftime(fmt)\n",
    "\n",
    "def currUtcTime():\n",
    "    return datetime.datetime.now(timezone('UTC')).strftime(fmt)\n",
    "\n",
    "def toUtc(time):\n",
    "    tz = pytz.timezone('UTC')\n",
    "    return tz.localize(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDukasTime(soup):\n",
    "    \"\"\"\n",
    "    Returns the extracted time as given on the DukasCopy webpage from where the DukasCopy data is being extracted from.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    soup (soup): The BeautifulSoup object containing the parsed HTML representation of the DukasCopy webpage.\n",
    "    \n",
    "    Returns:\n",
    "    date_time_obj (Datetime): Datetime object representing the time on the page when the data was taken.\n",
    "    \"\"\"\n",
    "    tdata = soup.find_all(\"span\", {\"id\": \"timeUpdate\"})\n",
    "    dtxt = tdata[0].text\n",
    "    \n",
    "    date_time_obj = datetime.datetime.strptime(dtxt, ' %a, %d %b %Y %H:%M:%S GMT')\n",
    "    \n",
    "    return date_time_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateData(NewFrame, filename):\n",
    "    \"\"\"\n",
    "    Updates the pickle file specified bt \"filepath\" by appending new data from the \"NewFrame\" dataframe to it.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    NewFrame (DataFrame): The dataframe which is to be appended to the end of the pickle file to update the pickle file with the new data.\n",
    "    filepath (str): The filepath of the pickle file where new dataframe data is to be appended.\n",
    "    \n",
    "    \"\"\"\n",
    "    if (not filename.endswith(\".pkl\")):\n",
    "        raise Exception(\"The filename specified must end with .pkl extension. The filename provided was: \" + filename)\n",
    "    \n",
    "    if not os.path.isfile(filename):\n",
    "        pd.to_pickle(NewFrame, filename)\n",
    "        \n",
    "    else:   \n",
    "        df = pd.read_pickle(filename)\n",
    "        df = df.append(NewFrame, sort=False)\n",
    "        df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for headless chrome\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "options.add_argument('--no-proxy-server')\n",
    "options.add_argument(\"--proxy-server='direct://'\")\n",
    "options.add_argument(\"--proxy-bypass-list=*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDukasCopyData(pathToChromeDriver):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame object containing the most recent Gold Data from the DukasCopy website.\n",
    "    Scrapes the data from the DukasCopy website before formatting and returning the data as a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    pathToChromeDriver (str): Path to the chromedriver executable to be used to open web pages.\n",
    "    \n",
    "    Returns:\n",
    "    DCDataFrame (DataFrame): A pandas DataFrame containing the most recent Gold Data from the DukasCopy website.\n",
    "    dataTime (Datetime): Datetime object representing the time at which data was fetched.\n",
    "    \"\"\"\n",
    "    browser = webdriver.Chrome(pathToChromeDriver, options=options)\n",
    "    browser.get(DCurl)\n",
    "    \n",
    "    DChtml = browser.page_source\n",
    "    soup = BeautifulSoup(DChtml, \"lxml\")\n",
    "    \n",
    "    # Finding the exact table with all the required data\n",
    "    data = soup.find_all(\"table\", {\"id\": \"list\"})\n",
    "    \n",
    "    # Creating and Modifying dataframe with the table \n",
    "    DCDataFrame = pd.read_html(str(data))[0]\n",
    "    \n",
    "    # Add a timestamp to the data\n",
    "    dataTime = toUtc(getDukasTime(soup))\n",
    "    DCDataFrame[\"timestamp\"] = dataTime\n",
    "    DCDataFrame = DCDataFrame.set_index(\"timestamp\")\n",
    "    \n",
    "    # Filter dataframe only to Gold Data\n",
    "    DCDataFrame = DCDataFrame[DCDataFrame[\"Live\"] == \"XAU/USD\"]\n",
    "    \n",
    "    browser.quit()\n",
    "    return DCDataFrame, dataTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchAndSaveData(chromeDriverPath, filepath=\"DukasGoldData.pkl\"):\n",
    "    \"\"\"\n",
    "    Main driver for the code - calls all methods to fetch and save gold data, and alert in case of surges.\n",
    "    \"\"\"\n",
    "    currData, timestamp = getDukasCopyData(chromeDriverPath)\n",
    "    updateData(currData, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(chromeDriverPath, filepath=\"DukasGoldData.pkl\", collectionTime=900):\n",
    "    \"\"\"\n",
    "    Fetches and saves the most recent gold data from DukasCopy at an interval of \"collectionTime\" seconds\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        fetchAndSaveData(chromeDriverPath)\n",
    "        time.sleep(collectionTime - 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
